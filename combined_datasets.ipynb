{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b30d8d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1105702, 2351) (195124, 2351) (433609, 2351)\n"
     ]
    }
   ],
   "source": [
    "# 1. citesti datele din .dat si creezi loaders\n",
    "\n",
    "import idk\n",
    "import feature_extractors\n",
    "import joblib\n",
    "\n",
    "# baga aici folderu cu fisierele .dat\n",
    "data_dir = \"F://big_dataset\"\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = feature_extractors.read_vectorized_features_2(data_dir)\n",
    "train_loader, val_loader, test_loader, scaler = idk.prepare_dataloaders(X_train[:1000], y_train[:1000], X_val[:1000], y_val[:1000], X_test[:1000], y_test[:1000], batch_size=128) # aici aproape imi pusca mie :(\n",
    "joblib.dump(scaler, \"scalers/scaler_big.pkl\")\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c948f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0444f16f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (932506872.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    python3 -m  pipreqs.pipreqs .\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python3 -m  pipreqs.pipreqs ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d2b0172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. creezi modelul (asta ar fi primul si daca e necesar mai fac unul)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Model_v1_BIG(nn.Module):\n",
    "    def __init__(self, input_dim=2351):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024), \n",
    "            nn.Dropout(0.5), \n",
    "            \n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),  \n",
    "           \n",
    "            nn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f9fa862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Training on Model_v1_BIG----------\n",
      "\n",
      "\n",
      "Model_v1_BIG(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=2351, out_features=2048, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Linear(in_features=1024, out_features=1, bias=True)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Epoch 1 | Train Loss: 0.7395 | Train Acc: 0.4920 | Val Loss: 1.0890 | Val Acc: 0.0240\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m Model_v1_BIG()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[1;32m---> 14\u001b[0m results \u001b[38;5;241m=\u001b[39m idk\u001b[38;5;241m.\u001b[39mtrain_model(model, train_loader, val_loader, device, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m0.0001\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/Model_BIG_v1.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m idk\u001b[38;5;241m.\u001b[39mplot_training_metrics(\n\u001b[0;32m     17\u001b[0m     results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_losses\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     18\u001b[0m     results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_losses\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel_BIG_v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/Model_BIG_v1.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32md:\\Licenta\\MalwareAnalysis_Models\\idk.py:120\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, device, num_epochs, lr, save_path)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m avg_val_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n\u001b[0;32m    119\u001b[0m     best_val_loss \u001b[38;5;241m=\u001b[39m avg_val_loss\n\u001b[1;32m--> 120\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), save_path)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    122\u001b[0m     early_stopping_threshold_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mf:\\Anaconda\\Lib\\site-packages\\torch\\serialization.py:965\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    964\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 965\u001b[0m         _save(\n\u001b[0;32m    966\u001b[0m             obj,\n\u001b[0;32m    967\u001b[0m             opened_zipfile,\n\u001b[0;32m    968\u001b[0m             pickle_module,\n\u001b[0;32m    969\u001b[0m             pickle_protocol,\n\u001b[0;32m    970\u001b[0m             _disable_byteorder_record,\n\u001b[0;32m    971\u001b[0m         )\n\u001b[0;32m    972\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mf:\\Anaconda\\Lib\\site-packages\\torch\\serialization.py:1266\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m   1264\u001b[0m         storage \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m   1265\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[1;32m-> 1266\u001b[0m zip_file\u001b[38;5;241m.\u001b[39mwrite_record(name, storage, num_bytes)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3. test + train\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import idk\n",
    "\n",
    "print(\"----------Training on Model_v1_BIG----------\\n\\n\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Model_v1_BIG()\n",
    "print(model)\n",
    "results = idk.train_model(model, train_loader, val_loader, device, 20, 0.0001, \"models/Model_BIG_v1.pth\")\n",
    "\n",
    "idk.plot_training_metrics(\n",
    "    results[\"train_losses\"],\n",
    "    results[\"val_losses\"],\n",
    "    results[\"train_accuracies\"],\n",
    "    results[\"val_accuracies\"],\n",
    "    model_name=\"Model_BIG_v1\"\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(\"models/Model_BIG_v1.pth\"))\n",
    "test_results = idk.evaluate_model_on_test(model, test_loader, device)\n",
    "\n",
    "idk.plot_roc_curve(test_results[\"labels\"], test_results[\"probs\"])\n",
    "idk.plot_confusion_matrix(test_results[\"labels\"], test_results[\"probs\"])\n",
    "idk.evaluate_at_fpr_thresholds(test_results[\"labels\"], test_results[\"probs\"], fpr_targets=[0.01, 0.001])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
