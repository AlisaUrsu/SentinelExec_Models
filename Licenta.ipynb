{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def stream_jsonl_to_parquet(jsonl_path, output_folder, batch_size=10000):\n",
    "    buffer = []\n",
    "    batch_num = 0\n",
    "\n",
    "    output_folder = Path(output_folder)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"Processing {jsonl_path}...\")\n",
    "\n",
    "    with open(jsonl_path, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                if obj.get(\"label\") in [0, 1]:\n",
    "                    buffer.append(obj)\n",
    "\n",
    "                if len(buffer) >= batch_size:\n",
    "                    df = pd.DataFrame(buffer)\n",
    "                    output_path = output_folder / f\"batch_{batch_num}.parquet\"\n",
    "                    df.to_parquet(output_path, index=False, engine=\"pyarrow\", compression='snappy')\n",
    "                    print(f\"  Saved batch {batch_num} with {len(df)} rows to {output_path}\")\n",
    "                    buffer.clear()\n",
    "                    batch_num += 1\n",
    "\n",
    "                    break\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"  Skipping line {i} due to JSON error: {e}\")\n",
    "\n",
    "    # Save any leftover rows\n",
    "    if buffer:\n",
    "        df = pd.DataFrame(buffer)\n",
    "        output_path = output_folder / f\"batch_{batch_num}.parquet\"\n",
    "        df.to_parquet(output_path, index=False, engine=\"pyarrow\", compression='snappy')\n",
    "        print(f\"  Saved final batch {batch_num} with {len(df)} rows to {output_path}\")\n",
    "\n",
    "    print(f\"Finished processing {jsonl_path}\")\n",
    "\n",
    "\n",
    "# Open and process the JSONL file\n",
    "input_file = \"F://ember//train_features_1.jsonl\"\n",
    "output_dir = \"F://ember//parquet_batches\"\n",
    "\n",
    "stream_jsonl_to_parquet(input_file, output_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_parquet(\"F://ember2018//parquet_batches_0//batch_0.parquet\")\n",
    "df = pd.read_parquet(\"F://ember//parquet_batches//batch_0.parquet\")\n",
    "row = df.iloc[0]\n",
    "\n",
    "print(df.columns.tolist())\n",
    "print(row[\"header\"][\"optional\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df[\"label\"].value_counts().plot(kind=\"bar\", title=\"Malware vs Benign Distribution\")\n",
    "plt.xticks(ticks=[0, 1], labels=[\"Benign (0)\", \"Malware (1)\"])\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(row[\"general\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"F://ember2018//train_features_1.jsonl\"\n",
    "output_dir = \"F://ember2018//parquet_batches_1\"\n",
    "\n",
    "stream_jsonl_to_parquet(input_file, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.read_parquet(\"F://ember2018//parquet_batches_1//batch_0.parquet\")\n",
    "df[\"label\"].value_counts().plot(kind=\"bar\", title=\"Malware vs Benign Distribution\")\n",
    "plt.xticks(ticks=[0, 1], labels=[\"Benign (0)\", \"Malware (1)\"])\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lief\n",
    "import numpy as np\n",
    "from feature_extractors.general_info import GeneralFileInfo\n",
    "from feature_extractors.header_info import HeaderFileInfo\n",
    "from feature_extractors.byte_entropy_histogram import ByteEntropyHistogram\n",
    "from feature_extractors.imports import ImportsInfo\n",
    "from feature_extractors.sections import SectionInfo\n",
    "\n",
    "\n",
    "# Replace with your actual path to an .exe or .dll\n",
    "#pe_path = \"C://Users//Alisa//Desktop//Unity app//CatKyu!! v3//CatKyu!!.exe\"\n",
    "pe_path = \"E://SteamLibrary//steamapps//common//Blasphemous 2//Blasphemous 2.exe\"\n",
    "# Read raw bytes\n",
    "with open(pe_path, \"rb\") as f:\n",
    "    bytez = f.read()\n",
    "\n",
    "# Try to parse with LIEF\n",
    "try:\n",
    "    lief_binary = lief.PE.parse(list(bytez))  # turn bytez into a list for LIEF\n",
    "except Exception as e:\n",
    "    print(f\"Failed to parse with LIEF: {e}\")\n",
    "    lief_binary = None\n",
    "\n",
    "\n",
    "# Initialize your feature extractor\n",
    "feat_extractor = GeneralFileInfo()\n",
    "head_extractor = HeaderFileInfo()\n",
    "entro = ByteEntropyHistogram()\n",
    "imports_extractor = ImportsInfo()\n",
    "sections_extractor = SectionInfo()\n",
    "\n",
    "# Get raw and processed features\n",
    "raw = feat_extractor.raw_features(bytez, lief_binary)\n",
    "vector = feat_extractor.process_raw_features(raw)\n",
    "\n",
    "# Print results\n",
    "print(\"Raw features:\")\n",
    "print(raw)\n",
    "\n",
    "print(\"\\nProcessed vector:\")\n",
    "print(vector)\n",
    "print(\"Shape:\", vector.shape)\n",
    "\n",
    "\n",
    "raw = head_extractor.raw_features(bytez, lief_binary)\n",
    "vector = head_extractor.process_raw_features(raw)\n",
    "\n",
    "print(\"Raw features:\")\n",
    "print(raw)\n",
    "\n",
    "print(\"\\nProcessed vector:\")\n",
    "print(vector)\n",
    "print(\"Shape:\", vector.shape)\n",
    "\n",
    "raw = entro.raw_features(bytez, lief_binary)\n",
    "vector = entro.process_raw_features(raw)\n",
    "\n",
    "print(\"Raw features:\")\n",
    "print(raw)\n",
    "\n",
    "print(\"\\nProcessed vector:\")\n",
    "print(vector)\n",
    "print(\"Shape:\", vector.shape)\n",
    "\n",
    "raw = imports_extractor.raw_features(bytez, lief_binary)\n",
    "vector = imports_extractor.process_raw_features(raw)\n",
    "\n",
    "print(\"Raw features:\")\n",
    "print(raw)\n",
    "\n",
    "print(\"\\nProcessed vector:\")\n",
    "print(vector)\n",
    "print(\"Shape:\", vector.shape)\n",
    "\n",
    "raw = sections_extractor.raw_features(bytez, lief_binary)\n",
    "vector = sections_extractor.process_raw_features(raw)\n",
    "\n",
    "print(\"Raw features:\")\n",
    "print(raw)\n",
    "\n",
    "print(\"\\nProcessed vector:\")\n",
    "print(vector)\n",
    "print(\"Shape:\", vector.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector shape: (2351,)\n",
      "SHA256 of file: 5da46b251971f2ce199029648370a085e9f96c590d1383efc8b12365e6f5fcb0\n",
      "First 10 values of vector: [0.23798606 0.04504423 0.03002949 0.02433224 0.02012315 0.01815932\n",
      " 0.01617102 0.01413224 0.01381717 0.01223724 0.01147098 0.01051506\n",
      " 0.00966009 0.00922114 0.00881277 0.00958515 0.00900854 0.00795474\n",
      " 0.00724507 0.00740872 0.00726036 0.00705695 0.0061928  0.00595726\n",
      " 0.00618362 0.00573549 0.00553207 0.00503805 0.00480404 0.00458533\n",
      " 0.00423508 0.00406226 0.00530418 0.00374413 0.00374872 0.00352388\n",
      " 0.00456392 0.00317975 0.00287233 0.00278821 0.00302069 0.00250679\n",
      " 0.00230796 0.0024242  0.00222537 0.00284633 0.00224066 0.00203266\n",
      " 0.00238902 0.00192407 0.00185677 0.00226666 0.00183689 0.001687\n",
      " 0.00168088 0.00153864 0.00190418 0.00167323 0.00164876 0.00208925\n",
      " 0.00159217 0.00156311 0.00187359 0.00216878 0.00305281 0.00212595\n",
      " 0.00135816 0.00142546 0.001843   0.00165335 0.00121286 0.0012404\n",
      " 0.00660575 0.0018583  0.00109968 0.00120522 0.0019623  0.00142087\n",
      " 0.00101098 0.00098192 0.00126793 0.00093144 0.00100486 0.00108133\n",
      " 0.00118839 0.00116239 0.00096815 0.00110886 0.00098956 0.00086721\n",
      " 0.00087638 0.00091003 0.00115933 0.00087944 0.00080297 0.00099415\n",
      " 0.00101862 0.00139181 0.00079532 0.0011731 ]\n"
     ]
    }
   ],
   "source": [
    "from feature_extractors.pe_feature import PEFeatureExtractor\n",
    "\n",
    "\n",
    "pe_path = \"E://SteamLibrary//steamapps//common//Blasphemous 2//Blasphemous 2.exe\"\n",
    " # Assuming your class is saved in this file\n",
    "\n",
    "extractor = PEFeatureExtractor()\n",
    "\n",
    "# Load a PE (.exe) file\n",
    "with open(pe_path, \"rb\") as f:\n",
    "    bytez = f.read()\n",
    "\n",
    "# Extract the raw feature dict\n",
    "raw = extractor.raw_features(bytez)\n",
    "\n",
    "# Turn it into a flat vector (normalized)\n",
    "vector = extractor.process_raw_features(raw)\n",
    "\n",
    "print(\"Feature vector shape:\", vector.shape)\n",
    "print(\"SHA256 of file:\", raw['sha256'])\n",
    "print(\"First 10 values of vector:\", vector[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from feature_extractors.pe_feature import PEFeatureExtractor\n",
    "df = pd.read_parquet(\"F://ember//parquet_batches//batch_0.parquet\")\n",
    "\n",
    "    # Create the extractor\n",
    "extractor = PEFeatureExtractor()\n",
    "\n",
    "feature_vectors = []\n",
    "vectors = []\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    try:\n",
    "        # row is a Series (like a dict), compatible with extractor\n",
    "        vector = extractor.process_raw_features(row)\n",
    "        vectors.append(vector)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        vectors.append(None)  # or skip\n",
    "\n",
    "# Filter out any None values (optional)\n",
    "vectors = [v for v in vectors if v is not None]\n",
    "idk = np.array(vectors, dtype=np.float32)\n",
    "print(idk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist())\n",
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "import tqdm \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from feature_extractors.pe_feature import PEFeatureExtractor\n",
    "\n",
    "\n",
    "def raw_feature_iterator(file_paths):\n",
    "    \"\"\"\n",
    "    Yield raw feature strings from the inputed file paths\n",
    "    \"\"\"\n",
    "    for path in file_paths:\n",
    "        with open(path, \"r\") as fin:\n",
    "            for line in fin:\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                    if obj.get(\"label\") in (0, 1):\n",
    "                        yield line\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping invalid line: {e}\")\n",
    "\n",
    "def count_filtered_lines(file_paths):\n",
    "    count = 0\n",
    "    for _ in raw_feature_iterator(file_paths):\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def write_filtered_features(raw_feature_paths, output_path):\n",
    "    with open(output_path, \"w\") as fout:\n",
    "        for fp in raw_feature_paths:\n",
    "            with open(fp, \"r\") as fin:\n",
    "                for line in fin:\n",
    "                    try:\n",
    "                        obj = json.loads(line)\n",
    "                        if obj.get(\"label\") in (0, 1):\n",
    "                            fout.write(line)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Skipping invalid line: {e}\")\n",
    "\n",
    "\n",
    "def vectorize(irow, raw_features_string, X_path, y_path, extractor, nrows):\n",
    "    \"\"\"\n",
    "    Vectorize a single sample of raw features and write to a large numpy file\n",
    "    \"\"\"\n",
    "    raw_features = json.loads(raw_features_string)\n",
    "    feature_vector = extractor.process_raw_features(raw_features)\n",
    "\n",
    "    y = np.memmap(y_path, dtype=np.float32, mode=\"r+\", shape=nrows)\n",
    "    y[irow] = raw_features[\"label\"]\n",
    "\n",
    "    X = np.memmap(X_path, dtype=np.float32, mode=\"r+\", shape=(nrows, extractor.dim))\n",
    "    X[irow] = feature_vector\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def vectorize_unpack(args):\n",
    "    \"\"\"\n",
    "    Pass through function for unpacking vectorize arguments\n",
    "    \"\"\"\n",
    "    return vectorize(*args)\n",
    "\n",
    "def vectorize_subset(X_path, y_path, raw_feature_paths, extractor, nrows):\n",
    "    \"\"\"\n",
    "    Vectorize a subset of data and write it to disk\n",
    "    \"\"\"\n",
    "    # Create space on disk to write features to\n",
    "    X = np.memmap(X_path, dtype=np.float32, mode=\"w+\", shape=(nrows, extractor.dim))\n",
    "    y = np.memmap(y_path, dtype=np.float32, mode=\"w+\", shape=nrows)\n",
    "    del X, y\n",
    "\n",
    "    # Distribute the vectorization work\n",
    "    pool = multiprocessing.Pool()\n",
    "    argument_iterator = ((irow, raw_features_string, X_path, y_path, extractor, nrows)\n",
    "                         for irow, raw_features_string in enumerate(raw_feature_iterator(raw_feature_paths)))\n",
    "    for args in tqdm.tqdm(argument_iterator, total=nrows):\n",
    "        vectorize_unpack(args)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_vectorized_features(data_dir):\n",
    "    \"\"\"\n",
    "    Create feature vectors from raw features and write them to disk\n",
    "    \"\"\"\n",
    "    extractor = PEFeatureExtractor()\n",
    "    raw_feature_paths = [os.path.join(data_dir, f\"output.jsonl\") for i in range(1)]\n",
    "    filtered_path = os.path.join(data_dir, \"filtered_train.jsonl\")\n",
    "\n",
    "    # Only run this once if needed\n",
    "    write_filtered_features(raw_feature_paths, filtered_path)\n",
    "\n",
    "    # Use the filtered file\n",
    "    nrows = sum(1 for _ in open(filtered_path)) \n",
    "    print(\"Vectorizing training set\")\n",
    "    X_path = os.path.join(data_dir, \"X.dat\")\n",
    "    y_path = os.path.join(data_dir, \"y.dat\")\n",
    "    #raw_feature_paths = [os.path.join(data_dir, \"output.jsonl\".format(i)) for i in range(1)]\n",
    "    #nrows = count_filtered_lines(raw_feature_paths)\n",
    "    vectorize_subset(X_path, y_path, raw_feature_paths, extractor, nrows)\n",
    "\n",
    "\n",
    "def read_vectorized_features(data_dir, subset=None):\n",
    "    \"\"\"\n",
    "    Read vectorized features into memory mapped numpy arrays\n",
    "    \"\"\"\n",
    "    if subset is not None and subset not in [\"train\", \"test\"]:\n",
    "        return None\n",
    "\n",
    "    extractor = PEFeatureExtractor()\n",
    "    ndim = extractor.dim\n",
    "    X_train = None\n",
    "    y_train = None\n",
    "    X_test = None\n",
    "    y_test = None\n",
    "\n",
    "    if subset is None or subset == \"train\":\n",
    "        X_train_path = os.path.join(data_dir, \"X.dat\")\n",
    "        y_train_path = os.path.join(data_dir, \"y.dat\")\n",
    "        y_train = np.memmap(y_train_path, dtype=np.float32, mode=\"r\")\n",
    "        N = y_train.shape[0]\n",
    "        X_train = np.memmap(X_train_path, dtype=np.float32, mode=\"r\", shape=(N, ndim))\n",
    "        if subset == \"train\":\n",
    "            return X_train, y_train\n",
    "\n",
    "    \n",
    "    return X_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7019/7019 [00:09<00:00, 757.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7019, 2351)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feature_extractors\n",
    "data_dir = \"F://ember//idk\"\n",
    "\n",
    "feature_extractors.create_vectorized_features(data_dir)\n",
    "X_train, y_train = feature_extractors.read_vectorized_features(data_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
